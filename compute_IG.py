# implement "predictions_and_gradients" function and follow
# https://github.com/ankurtaly/Integrated-Gradients/blob/master/IntegratedGradients/integrated_gradients.py
# for integrated gradients implementations : https://arxiv.org/abs/1703.01365

from torch.utils.data import DataLoader, Dataset
import os
from PIL import Image


def my_preprocess():
    """
    Taken from https://github.com/openai/CLIP/blob/main/clip/clip.py#L79C2-L79C2 ,
    with the normalisation step removed because it creates artifacts which make it difficult to
    visualise the image. The following is the original implementation:
    def _transform(n_px):
    return Compose([
        Resize(n_px, interpolation=BICUBIC),
        CenterCrop(n_px),
        _convert_image_to_rgb,
        ToTensor(),
        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),
    ])
    :return: composition of preprocessing steps
    """
    from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize
    from torchvision.transforms import InterpolationMode
    BICUBIC = InterpolationMode.BICUBIC

    def _convert_image_to_rgb(image):
        return image.convert("RGB")

    return Compose([
        Resize(224, interpolation=BICUBIC),
        CenterCrop(224),
        _convert_image_to_rgb,
        ToTensor()
    ])


def predictions_and_gradients(scaled_inputs, target_label, model, loss_function):
    """
    :param scaled_inputs: List of tensors of the same shape as 'input'.
    List of torch tensor (batchsize, height, width, channels)
    :param target_label

    :return:
    - predictions: Predicted probability distribution across all classes
          for each input. It has shape <batch, num_classes> where 'batch' is the
          number of inputs and num_classes is the number of classes for the model.
      - gradients: Gradients of the prediction for the target class (denoted by
          target_label) with respect to the inputs. It has the same shape
          as 'inputs'.
    """
    import torch
    def get_grad_pred(single_input, label):
        """
        :param single_input: torch tensor
        :param label:
        :return:
        """
        torch.autograd.set_detect_anomaly(True)
        x = single_input.detach().clone()
        x.requires_grad = True
        criterion = loss_function
        loss = criterion(model(x), label)
        single_grad = torch.autograd.grad(loss, x, allow_unused=True)[0]
        with torch.no_grad():
            single_pred = model(single_input)
        single_pred_numpy = single_pred.detach().cpu().numpy()
        single_grad_numpy = single_grad.detach().cpu().numpy()
        return single_grad_numpy, single_pred_numpy

    predictions = [get_grad_pred(single_input=single_input, label=target_label)[1] for single_input in scaled_inputs]
    gradients = [get_grad_pred(single_input=single_input, label=target_label)[0] for single_input in scaled_inputs]

    return predictions, gradients


def integrated_gradients(inp, target_label, baseline, model, predictions_and_gradients, loss_function, steps=50):
    """
    Documention taken from
    "https://github.com/ankurtaly/Integrated-Gradients/blob/master/IntegratedGradients/integrated_gradients.py"
    Computes integrated gradients for a given network and prediction label.

    Integrated gradients is a technique for attributing a deep network's
    prediction to its input features. It was introduced by:
    https://arxiv.org/abs/1703.01365

    In addition to the integrated gradients tensor, the method also
    returns some additional debugging information for sanity checking
    the computation. See sanity_check_integrated_gradients for how this
    information is used.

    This method only applies to classification networks, i.e., networks
    that predict a probability distribution across two or more class labels.

    Access to the specific network is provided to the method via a
    'predictions_and_gradients' function provided as argument to this method.
    The function takes a batch of inputs and a label, and returns the
    predicted probabilities of the label for the provided inputs, along with
    gradients of the prediction with respect to the input. Such a function
    should be easy to create in most deep learning frameworks.

    Args:
      inp: The specific input for which integrated gradients must be computed.
      target_label_index: Index of the target class for which integrated gradients
        must be computed.
      baseline: [optional] The baseline input used in the integrated
        gradients computation. If None (default), the all zero tensor with
        the same shape as the input (i.e., 0*input) is used as the baseline.
        The provided baseline and input must have the same shape.
      steps: [optional] Number of intepolation steps between the baseline
        and the input used in the integrated gradients computation. These
        steps along determine the integral approximation error. By default,
        steps is set to 50.

    Returns:
      integrated_gradients: The integrated_gradients of the prediction for the
        provided prediction label to the input. It has the same shape as that of
        the input.

      The following output is meant to provide debug information for sanity
      checking the integrated gradients computation.
      See also: sanity_check_integrated_gradients

      prediction_trend: The predicted probability distribution across all classes
        for the various (scaled) inputs considered in computing integrated gradients.
        It has shape <steps, num_classes> where 'steps' is the number of integrated
        gradient steps and 'num_classes' is the number of target classes for the
        model.
    """
    import numpy as np
    if baseline is None:
        baseline = 0*inp
    assert(baseline.shape == inp.shape)

    # Scale input and compute gradients.
    scaled_inputs = [baseline + (float(i)/steps)*(inp-baseline) for i in range(0, steps+1)]
    predictions, grads = predictions_and_gradients(scaled_inputs, target_label, model, loss_function)  # shapes: <steps+1>, <steps+1, inp.shape>

    # Use trapezoidal rule to approximate the integral.
    # See Section 4 of the following paper for an accuracy comparison between
    # left, right, and trapezoidal IG approximations:
    # "Computing Linear Restrictions of Neural Networks", Matthew Sotoudeh, Aditya V. Thakur
    # https://arxiv.org/abs/1908.06214
    # mistake from the original implementation has been corrected by the following line of code
    grads_intermediate = [0.5*(x+y) for x, y in zip(grads[:-1], grads[1:])]
    avg_grads = np.average(grads_intermediate, axis=0)
    inp = inp.detach().cpu().numpy()
    baseline = baseline.detach().cpu().numpy()
    integrated_grads = (inp-baseline)*avg_grads  # shape: <inp.shape>
    return integrated_grads, predictions


class CompressedDATASET(Dataset):
    """
    A Dataset object for processing compressed images.
    """
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.image_files = sorted(os.listdir(root_dir)) # return file names with a specific order
        self.labels = [0] * len(self.image_files)  # Initialize all labels to 0 (placeholder)

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_name = os.path.join(self.root_dir, self.image_files[idx])
        image = Image.open(image_name)

        if self.transform:
            image = self.transform(image)

        label = self.labels[idx]
        return image, label


def read_txt_to_list(file_load_path):
    try:
        with open(file_load_path, "r") as file:
            content = file.read()

        lines = content.splitlines()

        data_list = [int(line) for line in lines]

        if data_list is not None:
            print("List loaded successfully:")

        return data_list
    except Exception as e:
        print(f"An error occurred while reading the file: {e}")
        return None


def get_image(compression_quality, train_or_val, index, preprocess, device, dataset_name, data_storage_dir, batch_size=1):
    """
    Reading an image from a specified dataset.
    :return: preprocessed image
    """
    from torchvision import datasets, transforms
    import torch
    from torchvision.transforms import ToPILImage
    from os.path import join
    to_pil = ToPILImage()

    # compressed_images_dir = f"/media/cangxiong/storage/datasets/{dataset_name}_compressed/images_compr_{compression_quality}/{train_or_val}"
    compressed_images_dir = join(data_storage_dir,
                                 f"{dataset_name}_compressed/images_compr_{compression_quality}/{train_or_val}")
    compressed_dataset = CompressedDATASET(root_dir=compressed_images_dir, transform=transforms.ToTensor())

    # load the ground truth labels and update the compressed images
    # file_load_path = f"/media/cangxiong/storage/datasets/{dataset_name}_compressed/labels/{train_or_val}_labels.txt"
    file_load_path = join(data_storage_dir, f"{dataset_name}_compressed/labels/{train_or_val}_labels.txt")
    label_list = read_txt_to_list(file_load_path)
    compressed_dataset.labels = label_list
    data_sample = torch.utils.data.Subset(compressed_dataset, [index])
    data_loader = torch.utils.data.DataLoader(data_sample, batch_size=batch_size, shuffle=False)

    data_list = []
    for i, data in enumerate(data_loader):
        data_list.append(data)
    image = data_list[0][0]
    label = data_list[0][1]
    image_pil = to_pil(image.squeeze())
    image_trans = preprocess(image_pil).unsqueeze(0).to(device)
    return image_trans, label


def clip_get_score(input_data, model, device):
    """
    Computing the inner product between image and text vectors for each image class.
    """
    import clip
    import torch
    image_class = ["airplane", "automobile", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck"]
    text_inputs = torch.cat([clip.tokenize(f"This is a image of a {a}.") for a in image_class]).to(device)

    # Calculate features
    image_features = model.encode_image(input_data)
    with torch.no_grad():
        text_features = model.encode_text(text_inputs)

    image_features_normalised = image_features / image_features.norm(dim=-1, keepdim=True)
    text_features_normalised = text_features / text_features.norm(dim=-1, keepdim=True)
    similarity = 100.0 * image_features_normalised @ text_features_normalised.T
    return similarity


def main():
    import torch
    import argparse
    import numpy as np
    import clip
    import os
    parser = argparse.ArgumentParser()

    parser.add_argument("--out_dir", help="output directory")
    parser.add_argument("--test_name")
    parser.add_argument("--train_or_val")
    parser.add_argument("--index", type=int, help="index of the image from the dataset")
    parser.add_argument("--model_seed", type=int)
    parser.add_argument("--img_encoder")
    parser.add_argument("--dataset_name")
    parser.add_argument("--data_storage_dir")

    args = parser.parse_args()
    output_dir = args.out_dir
    test_name = args.test_name
    index = args.index
    train_or_val = args.train_or_val  # 'val'
    model_seed = args.model_seed  # 10
    img_encoder = args.img_encoder
    dataset_name = args.dataset_name
    data_storage_dir = args.data_storage_dir

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    torch.manual_seed(model_seed)
    np.random.seed(model_seed)

    # turn CLIP into a zero-shot image classifier
    if img_encoder in ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64']:
        clip_model, preprocess = clip.load(f'{img_encoder}', device)
    elif img_encoder == 'ViT-B-32':
        clip_model, preprocess = clip.load('ViT-B/32', device)
    elif img_encoder == 'ViT-B-16':
        clip_model, preprocess = clip.load('ViT-B/16', device)
    elif img_encoder == 'ViT-L-14':
        clip_model, preprocess = clip.load('ViT-L/14', device)
    elif img_encoder == 'ViT-L-14@336px':
        clip_model, preprocess = clip.load('ViT-L/14@336px', device)

    def model(input_data):
        """
        :param input_data: must be preprocessed, tensor format
        :return:
        """
        return clip_get_score(input_data, model=clip_model, device=device)

    # load one image with different qualities
    data_dict = {}
    label_list = {}
    compression_quality_list = [100, 75, 50, 25]
    for i in compression_quality_list:
        data, label = get_image(compression_quality=i, train_or_val=train_or_val, index=index, preprocess=my_preprocess(),
                                device=device, data_storage_dir=data_storage_dir, batch_size=1, dataset_name=dataset_name)

        data = data.to(device)
        label = label.to(device)
        data_dict[i] = data
        label_list[i] = label
    # compute the integrated gradients for the same image with different qualities
    baseline_data = data_dict[100]
    ig_dict = {}
    preds_dict = {}
    pred_label_list = {}
    pred_scores_at_true_label_list = {}
    data_dict_numpy = {}
    label_list_numpy = {}
    for i in compression_quality_list:
        grads_integrated, predictions = integrated_gradients(inp=data_dict[i], target_label=label_list[100], model=model,
                                                             predictions_and_gradients=predictions_and_gradients,
                                                             loss_function=torch.nn.CrossEntropyLoss(),
                                                             baseline=baseline_data, steps=50)
        pred_score_max = max(predictions[-1][0])
        pred_label = list(predictions[-1][0]).index(pred_score_max)
        # sanity check F(data) - F(baseline) should be equal to np.sum(grads_integrated); Note: there is a mistake in the
        # implementation of the paper for the trapezoidal approximation of the integral:
        # https://github.com/ankurtaly/Integrated-Gradients/blob/master/IntegratedGradients/integrated_gradients.py
        criterion = torch.nn.CrossEntropyLoss()
        with torch.no_grad():
            loss_data = criterion(model(data_dict[i]), label)
            loss_baseline = criterion(model(baseline_data), label)
            diff = loss_data - loss_baseline
            print(f"model(data) : {model(data_dict[i]).detach().cpu()}; true label : {label.detach().cpu()},"
                  f"predictions: {pred_label}\n")
        loss_data_value = loss_data.item()
        print(f"approximation : {np.sum(grads_integrated)}, loss_data : {loss_data_value}, "
              f"loss_baseline : {loss_baseline.item()}, diff : {diff.item()}")
        ig_dict[i] = grads_integrated.tolist()
        preds_dict[i] = predictions
        pred_label_list[i] = pred_label
        pred_scores_at_true_label_list[i] = np.exp(-loss_data_value)
        data_dict_numpy[i] = data_dict[i].detach().cpu().numpy().tolist()
        label_list_numpy[i] = label_list[i].detach().cpu().numpy().tolist()
    print("All integrated gradients have been computed!\n")

    # save the output for plotting
    import json
    ig_dict_save_path = os.path.join(output_dir, f'ig_dict_{test_name}_{index}_{img_encoder}_{dataset_name}.json')
    data_dict_save_path = os.path.join(output_dir, f'data_dict_{test_name}_{index}_{img_encoder}_{dataset_name}.json')
    pred_label_list_save_path = os.path.join(output_dir, f'pred_label_list_{test_name}_{index}_{img_encoder}_{dataset_name}.json')
    pred_scores_at_true_label_list_save_path = os.path.join(output_dir, f'pred_scores_at_true_label_list_{test_name}_{index}_{img_encoder}_{dataset_name}.json')
    label_list_save_path = os.path.join(output_dir, f'label_list_{test_name}_{index}_{img_encoder}_{dataset_name}.json')

    with open(ig_dict_save_path, 'w') as file:
        json.dump(ig_dict, file)

    with open(data_dict_save_path, 'w') as file:
        json.dump(data_dict_numpy, file)

    with open(pred_label_list_save_path, 'w') as file:
        json.dump(pred_label_list, file)

    with open(pred_scores_at_true_label_list_save_path, 'w') as file:
        json.dump(pred_scores_at_true_label_list, file)

    with open(label_list_save_path, 'w') as file:
        json.dump(label_list_numpy, file)

    print("All outputs have been saved for plotting!\n")


if __name__ == '__main__':
    main()