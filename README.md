# Understanding the Vulnerability of CLIP to Image Compression [[paper]](https://arxiv.org/abs/2311.14029)

## Abstract
CLIP([1]) is a widely used foundational vision-language model that is used for zero-shot image recognition and other
image-text alignment tasks. We demonstrate that CLIP is vulnerable to change in image quality under compression. 
This surprising result is further analysed using an attribution method-Integrated Gradients ([2]). 
Using this attribution method, we are able to better understand both quantitatively and qualitatively exactly the 
nature in which the compression affects the zero-shot recognition accuracy of this model. 
We evaluate this extensively on CIFAR-10 and STL-10. Our work provides the basis to understand this vulnerability of 
CLIP and can help us develop more effective methods to improve the robustness of CLIP and other vision-language models.

## Summary of main idea
Our main idea of probing using Integrated Gradient for zero-shot CLIP
can be illustrated as follows:
<p align="center">
    <img src="assets/probing_using_IG.png" width="80%" />
</p>

## Implementation
In this section, we will show how to reproduce the empirical results from our paper. 

First, create a virtual environment:
```
conda create -n clip 
```
Then follow https://github.com/openai/CLIP#usage and install CLIP:
```
conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0
pip install ftfy regex tqdm
pip install git+https://github.com/openai/CLIP.git
```
To prepare the compressed images, run the following command:
```
python dataset_compression.py --dataset cifar10 --crq 100 --train_or_val val --data_storage_dir your_preferred_directory
```
with "--crq" set to be 100, 75, 50 and 25. You can set "--dataset" to be "cifar10" or
"stl10".

After preparing the compressed data, we can compute the values of the integrated 
gradients. 
``` 
python compute_IG.py --out_dir your_output_directory --test_name test --train_or_val val \
--index 20 --model_seed 123 --img_encoder RN50 --dataset_name cifar10 \
--data_storage_dir directory_of_compressed_data
```
To get the comparison plots:
```
python plot_IG.py --data_dir directory_of_computed_IGs --output_dir your_output_directory \
--test_name test --index 20 --img_encoder RN50 --polarity both \
--dataset_name cifar10
```
An example output will look like this:
<p align="center">
    <img src="assets/comparisons_test_20_RN50_both_cifar10.png" width="100%" />
</p>


## Reference
[1] Radford, Alec, et al. "Learning transferable visual models from natural language supervision." 
International Conference on Machine Learning. PMLR, 2021.

[2] Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. 
"Axiomatic attribution for deep networks." International Conference on Machine Learning. PMLR, 2017.

## Citation
``` 
@inproceedings{chen2023understanding,
title={Understanding the Vulnerability of CLIP to Image Compression},
author={Cangxiong Chen and Vinay P. Namboodiri and Julian Padget},
booktitle={Workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models at NeurIPS 2023}
year={2023}
}
```

## License
This work is licensed under 
a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).